{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning sur big data en spark et TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scala, Accumulators, DataFrames and Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intérêts de Scala\n",
    "\n",
    "- robustesses des applications (i.e ne doit pas cacher de mauvaises suprrises), exemple python : typage dynamique donc erreurs potentielles à l'exécution (*runtime*) ;\n",
    "- performances : JVM, compilation en byte code, optimisation, etc. ;\n",
    "- concurrence : gestion des threads, etc. ;\n",
    "\n",
    "### Typage dynamique vs statique\n",
    "\n",
    "A la différence python qui est dynamiquement typé, Scala est statiquement typé.\n",
    "\n",
    "En scala, lorsque je crée une variable `x`, je dois lui attribuer un type, par exemple `Int`, `Double`, `String`, etc.\n",
    "Scala dispose d'un algorithme d'inférence de type qui permet de déduire le type de la variable `x` :\n",
    "\n",
    "```scala\n",
    "scala> val x = 42\n",
    "x: Int = 42\n",
    "```\n",
    "\n",
    "### Typage en Scala\n",
    "\n",
    "![Typage en scala.](typage_scala.png \"Typage en scala.\")\n",
    "\n",
    "**Attention il manque des inclusions sur cette image, par exemple, `Int` est un sous-ensemble de `Double`.**\n",
    "\n",
    "### `List`, `Array` and `Set` en Scala\n",
    "\n",
    "\n",
    "```scala\n",
    "scala> 8 * 5 + 2\n",
    "res0: Int = 42\n",
    "scala> 0.5 * res0\n",
    "res57: Double = 21.0\n",
    "scala> \"Hello, \" + res0\n",
    "res58: String = Hello, 42\n",
    "scala> 1 to 10\n",
    "res0: scala.collection.immutable.Range.Inclusive = Range 1 to 10\n",
    "// Range for populating sequences\n",
    "scala> val x = (1 to 10).toList // equivalent range in python\n",
    "x: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) //  List[int] : indique le type la valeur dans cette collection\n",
    "scala> val x = (1 to 10).toArray // dans la descente du gradient, on utilise des arrays\n",
    "x: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
    "scala> val x = (1 to 10).toSet // Set : collection d'éléments uniques, en l'occurrence il n'y a pas de doublons mais si on ajoute des éléments en doubles, il n'en prendra qu'un seul\n",
    "x: scala.collection.immutable.Set[Int] = Set(5, 10, 1, 6, 9, 2, 7, 3, 8, 4)\n",
    "```\n",
    "Différence entre `List`, `Array` et `Set` :\n",
    "- le nombre d'éléments est fixe dans une `Array` et on peut accéder à un élément par son index ;\n",
    "- les `array` sont stockés de façon contigue en mémoire, ce qui permet d'accéder à un élément en temps constant ;\n",
    "- `List` occupe beaucoup plus d'espaces en mémoire que `Array` car chaque élément de la liste est un objet qui contient la valeur et une référence vers l'élément suivant ;\n",
    "- Dans les `List` il faut suivres les pointeurs alors que pour `Array` on accède directement à l'élément ;\n",
    "- `List` est une collection immuable, on ne peut pas ajouter ou supprimer des éléments, on peut seulement créer une nouvelle liste en ajoutant ou supprimant des éléments ;\n",
    "- `List` : plus d'éfficacité pour ajouter ou supprimer des éléments en début de liste ;\n",
    "- `Array` : plus d'éfficacité pour accéder à un élément par son index et donc sur le plan de la mémoire ;\n",
    "- `Set` : collection d'éléments uniques, en l'occurrence il n'y a pas de doublons mais si on ajoute des éléments en doubles, il n'en prendra qu'un seul ;\n",
    "- `Set` est immutable, ordonné, propriété d'appartenance (*membership*) : avec des `Set` on peut vérifier si un élément appartient à un ensemble en temps quasi constant ;\n",
    "- **`Set` est plus rapide que `List` pour vérifier si un élément appartient à un ensemble ;**\n",
    "\n",
    "Si jamais l'on dispose d'une collection avec de sdoublons et l'on a besoin de vérifier l'appartenance d'un élément à cette collection, dans ce cas on peut utiliser un dictionnaire en python (`dict`) ou un `Set` en Scala.\n",
    "Les `Set` et les `dict` ont une structure équivalente, ils sont basés sur des tables de hachage (*hash tables*).\n",
    "\n",
    "faire une boucle for sur la liste\n",
    "Créer une liste de dictinnaires associés à chaque élément\n",
    "parcourir la liste et incrémenter le compteur de chaque élément\n",
    "```scala\n",
    "scala> val x = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
    "x: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
    "scala> val y = x.map(i => (i, 1))\n",
    "y: List[(Int, Int)] = List((1,1), (2,1), (3,1), (4,1), (5,1), (6,1), (7,1), (8,1), (9,1), (10,1))\n",
    "scala> val z = y.groupBy(_._1).mapValues(_.size)\n",
    "z: scala.collection.immutable.Map[Int,Int] = Map(5 -> 1, 10 -> 1, 1 -> 1, 6 -> 1, 9 -> 1, 2 -> 1, 7 -> 1, 3 -> 1, 8 -> 1, 4 -> 1)\n",
    "```\n",
    "\n",
    "### Conditional expressions\n",
    "```scala\n",
    "scala> val x=1\n",
    "x: Int = 1\n",
    "scala> if (x > 0) 1 else -1\n",
    "res1: Int = 1\n",
    "scala> val s = if (x > 0) 1 else -1\n",
    "s: Int = 1\n",
    "```\n",
    "\n",
    "### Block expressions and assignements\n",
    "```scala\n",
    "scala> { r = r * n; n -= 1 }\n",
    "scala> var c = { r = r * n; n -= 1 }\n",
    "c: Unit = ()\n",
    "scala>\n",
    "scala> var x0,y0=0.1\n",
    "x0: Double = 0.1\n",
    "y0: Double = 0.1\n",
    "scala> { val dx = x - x0; val dy = y - y0; sqrt(dx * dx + dy * dy) } // le type de cette expression est le type associé à la dernière valeur de l'expression : dx * dx + dy * dy qui est une racine carrée donc Double\n",
    "res40: Double = 1.2727922061357855\n",
    "```\n",
    "\n",
    "### Loops\n",
    "```scala\n",
    "scala> while (n > 0) { r=r* n\n",
    "        | n -= 1\n",
    "        | }\n",
    "scala> for (i <- 1 to n) // il n'y pas de in comme en python mais plutôt un <- (on peut parcourir des List, Arrays, Sets, etc.). Il s'agit d'une structure parraisseuse (lazy). On peut aussi utiliser until pour exclure la dernière valeur.\n",
    "        | r=r* i\n",
    "scala> val s = \"Hello\"\n",
    "s: String = Hello\n",
    "scala> var sum = 0\n",
    "sum: Int = 0\n",
    "scala> for (i <- 0 until s.length) // Last value for i is s.length - 1\n",
    "        | sum += s(i)\n",
    "```\n",
    "\n",
    "### Functions\n",
    "\n",
    "```scala\n",
    "scala> def abs(x: Double) = if (x >= 0) x else -x\n",
    "abs: (x: Double)Double // Type de la fonction : Double et de la sortie de la fonction : Double. Remarque on peut mettre le type de sortie de la fonction mais il faut prendre garde à ce que le type de sortie soit le même que le type de la valeur de retour de la fonction. Si l'on met un int, cela fonctionnera car int est un sous-ensemble de double.\n",
    "scala> def fac(n : Int) = {\n",
    "      | var r = 1\n",
    "      | for (i <- 1 to n) r = r * i\n",
    "      | r\n",
    "      | }\n",
    "fac: (n: Int)Int\n",
    "scala>\n",
    "scala> def fac(n: Int):Int =\n",
    "      | if (n <= 0) 1 else n * fac(n - 1) //fonction récursive\n",
    "fac: (n: Int)Int\n",
    "scala>\n",
    "```\n",
    "\n",
    "Lorsque l'on dispose d'une fonction récursive, le typage est obligatoire.\n",
    "\n",
    "### Pattern matching\n",
    "\n",
    "#### Rappels concernant les Patterns :\n",
    "- Motifs ;\n",
    "- En général lorsqu'il est question de pattern, il s'agit de valeurs que l'on peut réprésenter dans un langage de programation avec des parties qui ne sont pas complètement sépécifiées. Et qu'il faudra spécifier lorsque l'on fera le *pattern matching*.\n",
    "\n",
    "**exemple :**\n",
    "\n",
    "Ce pattern contient des éléments qui ne sont pas spécifiés. Si l'on dispose ce pattern : (a, 3), on va vérifier si le tuple : (7,3) correspond à ce pattern. Si c'est le cas, on va extraire la valeur de `a`. en scala, il est même possible de faire du pattern matching sur des types de données complexes comme des listes, des arbres, etc.\n",
    "\n",
    "```scala\n",
    "scala> val x = (1, 2, 3)\n",
    "x: (Int, Int, Int) = (1,2,3)\n",
    "scala> val (a, b, c) = x\n",
    "a: Int = 1\n",
    "b: Int = 2\n",
    "c: Int = 3\n",
    "\n",
    "scala> def fact(n: Int): Int = n match {\n",
    "      | case 0 => 1 // si n = 0 on retourne 1\n",
    "      | case n => n * fact(n - 1) // sinon on retourne n * fact(n - 1)\n",
    "      | }\n",
    "fact: (n: Int)Int\n",
    "```\n",
    "\n",
    "`case` va avec `match` mais la réciproque n'est pas garantie (*i.e.* `match` peut être utilisé sans `case`).\n",
    "\n",
    "```scala\n",
    "scala> r.map(c => c._1) //on veut retourner la première composante de c. equivalent en python : lambda c: c[0]\n",
    "scala> (k, v).map(case(k,v) => k) // équivalent à r.map(c => c._1). Dans ce cas cela fonctionne systématiquement et renvoie le premier élément du tuple. Si la valeur d'entrée ne correspond pas au case un erreur est levée en indiquant qu'il manque un case.\n",
    "\n",
    "scala> def f(x: Any): String = x match {\n",
    "      | case i:Int => \"integer: \" + i // pattern qui décrit une composante typage. Cela match si x est un entier\n",
    "      | case _:Double => \"a double\" // pattern qui match si x est un double\n",
    "      | case s:String => \"I want to say \" + s // pattern qui match si x est une chaine de caractères\n",
    "      | case _: Any => \"I do not know\"} // pattern qui match si x est de n'importe quel autre type que ceux précédemment cités\n",
    "f: (x: Any)String\n",
    "scala> f(3)\n",
    "res2: String = integer: 3\n",
    "scala> f(())\n",
    "res3: String = I do not know\n",
    "scala> f(8.6)\n",
    "res4: String = a double\n",
    "scala> f(\"Hey\")\n",
    "res5: String = I want to say Hey\n",
    "```\n",
    "\n",
    "### Variable-Length Arrays\n",
    "\n",
    "```scala\n",
    "scala> import scala.collection.mutable.ArrayBuffer\n",
    "scala> val b = ArrayBuffer[Int]()\n",
    "b: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer()\n",
    "scala> b += 1 // On ajoute un élément à l'ArrayBuffer b\n",
    "res17: b.type = ArrayBuffer(1)\n",
    "scala> b += (1, 2, 3, 5)\n",
    "res18: b.type = ArrayBuffer(1, 1, 2, 3, 5)\n",
    "scala> b ++= Array(8, 13, 21) // Ajout d'une séquence à une ArrayBuffer. En l'occurrence on ajoute un Array à l'ArrayBuffer. Dans ce cas c'est possible par l'intermédiaire de l'opérateur ++= mais ce n'est pas toujours le ca.\n",
    "res19: b.type = ArrayBuffer(1, 1, 2, 3, 5, 8, 13, 21)\n",
    "scala> b.trimEnd(5)\n",
    "scala> b\n",
    "res27: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(1, 1, 2)\n",
    "```\n",
    "\n",
    "### Traversing Arrays\n",
    "\n",
    "```scala\n",
    "scala> b ++= Array(8, 13, 21)\n",
    "res19: b.type = ArrayBuffer(1, 1, 2, 3, 5, 8, 13, 21)\n",
    "scala> b.trimEnd(5)\n",
    "scala> for (i <- 0 until b.length)\n",
    "      | println(i + \": \" + b(i))\n",
    "0: 1\n",
    "1: 1\n",
    "2: 2\n",
    "scala> for (elem <- b)\n",
    "      | println(elem)\n",
    "1\n",
    "1\n",
    "2\n",
    "```\n",
    "### Transforming Arrays\n",
    "\n",
    "```scala\n",
    "scala> b ++= Array(8, 13, 21)\n",
    "res19: b.type = ArrayBuffer(1, 1, 2, 3, 5, 8, 13, 21)\n",
    "scala> b.trimEnd(5)\n",
    "scala> for (elem <- b if elem % 2 == 0) yield 2 * elem //yield permet de retourner une valeur et génère à la demande cet élément. Il s'agit d'une opération lazy.\n",
    "res25: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(4)\n",
    "scala> b.filter(x => x % 2 == 0 ).map(x => 2 * x) // en remplaçant b par un rdd cela devient du code spark et le code sera exécuté de manière distribuée. Ce n'est pas la cas pour for (elem <- b if elem % 2 == 0) yield 2 * elem\n",
    "res26: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(4)\n",
    "scala> b.filter(_ % 2 == 0).map(2 * _) // Losqu'il s'agit d'un seul élément on peut utiliser _ à la place de x. Cela permet de simplifier le code.\n",
    "res27: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(4)\n",
    "```\n",
    "\n",
    "Remarque : L'équivalent `lambda x :` est `x =>` en Scala.\n",
    "\n",
    "### Quelques éléments supplémentaires\n",
    "\n",
    "```scala\n",
    "scala> // calculer la somme de toutes les valeurs de b\n",
    "scala> b.reduce((x, y) => x + y) // en python lambda x, y : x + y. reduce est une opération qui prend une fonction à deux arguments et qui va appliquer cette fonction à chaque élément de la liste. Cela va donner un résultat final.\n",
    "scala> b = ArrayBuffer(1, 1, 2)\n",
    "scala> b.zip(b) // prende deux collections de même longueur est va créer une collection de tuples. Dans ce cas on a une collection de tuples de la forme (1, 1), (1, 1), (2, 2). Il faut que les collections aient la même longueur.\n",
    "scala> b.zipWithIndex // va donner une collection de tuples de la forme (1, 0), (1, 1), (2, 2). Cela permet de donner un index à chaque élément de la collection.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descente du gradient\n",
    "\n",
    "### Premiers pas\n",
    "\n",
    "```scala\n",
    "scala> val v1 = Array(1.0, 1.0)\n",
    "scala> val v2 = Array(1.0, 1.0)\n",
    "scala> // somme, produit, scalaire, soustraction, produit par un scalaire\n",
    "scala> // Exemples :\n",
    "scala> // Somme : sum(v1, v1) ----> (2.0, 2.0)\n",
    "scala> def sum(v1: Array[Double], v2:Array[Double]):Array[Double] = {\n",
    "  (v1 zip v2).map{case (a, b) => a + b}\n",
    "}\n",
    "scala> sum(v1, v1)\n",
    "scala> res0: Array[Double] = Array(2.0, 2.0)\n",
    "scala> // Produit : prodScal(v1,v2) ------> 2\n",
    "scala> // sub(v1, v2) ------> (0,0)\n",
    "scala> // prodByscal(5, v1) ------> (5.0, 5.0)\n",
    "\n",
    "\n",
    "scala> val v1 = Array(1.0, 1.0)\n",
    "scala> val v2 = Array(1.0, 1.0)\n",
    "scala> // somme, produite, scalaire, soustraction, produit par un scalaire\n",
    "scala> // Exemples :\n",
    "scala> // Somme : sum(v1, v1) ----> (2.0, 2.0)\n",
    "scala> def sum(v1: Array[Double], v2: Array[Double]) = {\n",
    "    | val n = v1.length\n",
    "    | val res = new Array[Double](n)\n",
    "    | for (i <- 0 until n) \n",
    "    | | res(i) = v1(i) + v2(i)\n",
    "    | | res\n",
    "    | };\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descente du gradient\n",
    "\n",
    "\n",
    "![Régression linéaire.](Linear_regression.png \"Régression linéaire.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f_{w}(x) = w \\cdot \\phi(x)$ (avec ou sans biais ou avec de l'*offset* ou sans *offset*).\n",
    "\n",
    "Guider l'apprentissage de façon à ce ques les écarts entre les valeurs prédites et les valeurs réelles soient minimisés ($(w \\cdot \\phi(x)) - y$).\n",
    "\n",
    "Régression $L_2$ :\n",
    "$\\text{Loss}_{\\text{squared}}(x,y,\\textbf{w}) = (w \\cdot \\phi(x) - y)^{2}$.\n",
    "- La valeur de $\\textbf{w}$ qui minimise la fonction de perte est la moyenne de $y$ ;\n",
    "- La moyenne s'accomode à l'ensemble des exemples.\n",
    "\n",
    "$\\text{Loss}_{\\text{absdev}}(x,y,\\textbf{w}) = \\vert w \\cdot \\phi(x) - y \\vert$.\n",
    "- La valeur de $\\textbf{w}$ qui minimise la fonction de perte est la médiane de $y$ ;\n",
    "- La médiane est plus robuste aux valeurs extrêmes.\n",
    "\n",
    "### Gradient\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{TrainLoss}(\\textbf{w}) &= \\frac{1}{\\vert D_{train} \\vert} \\sum_{i=1}^{n} \\text{Loss}(x_i, y_i, \\textbf{w}) \\\\\n",
    "&= \\min\\limits_{\\textbf{w} \\in \\mathbb{R}^{d}} \\text{TrainLoss}(\\textbf{w})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "1. Initialize $\\textbf{w} = [0, \\cdots, 0]$ ;\n",
    "2. For $\\text{t} \\in [1, \\cdots, T]$:\n",
    "    - $\\textbf{w} \\leftarrow \\textbf{w} - \\underbrace{\\eta}_{\\text{step size}} \\nabla_{w} \\underbrace{\\text{TrainLoss}(\\textbf{w})}_{\\text{gradient}}$\n",
    "    - $\\eta$ : *step size* ;\n",
    "    - $\\nabla_{\\textbf{w}} \\text{TrainLoss}(\\textbf{w})$ : *gradient* de la fonction de perte.\n",
    "\n",
    "Spark permet de faire du parallélisme, donc la combinaison des hyperparamètres comme $\\eta$, est possible.\n",
    "\n",
    "Il est possible que la loss ne soit pas concave notamment en *deep learning*. C'est la raison pour laquelle est utilisée la descente du gradient stochastique (adaptatif).\n",
    "Par exemple : *Adam* (qui regarde une partie du passé), *Adagrad*, *RMSprop*, etc.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
